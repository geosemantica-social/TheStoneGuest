"""
TEST VALIDACI√ìN 2.8B, 6.9B, 12B - PYTHIA
Confirmaci√≥n de transiciones T2, T3, T4
Optimizado para Google Colab T4 (16GB VRAM)
Tiempo estimado: ~2 horas

INSTRUCCIONES:
1. Ejecutar en Colab con GPU T4
2. Primera celda: !pip install -q -U transformers accelerate bitsandbytes scipy
3. Runtime > Restart runtime
4. Ejecutar este script
"""

import torch
import numpy as np
from transformers import AutoModel, AutoTokenizer, BitsAndBytesConfig
from scipy import stats
import gc
import warnings
warnings.filterwarnings('ignore')

# ============================================
# CONFIGURACI√ìN PARA T4 (CR√çTICO)
# ============================================
class OptimizedSemanticAnalyzer:
    """
    Versi√≥n ultra-optimizada para T4 con 16GB
    """
    def __init__(self, model_name: str, use_8bit: bool = True):
        print(f"üîß Cargando {model_name}...")
        
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # SINTAXIS MODERNA de cuantizaci√≥n
        try:
            if use_8bit:
                quantization_config = BitsAndBytesConfig(
                    load_in_8bit=True,
                    llm_int8_threshold=6.0
                )
                self.model = AutoModel.from_pretrained(
                    model_name,
                    quantization_config=quantization_config,
                    device_map="auto",
                    torch_dtype=torch.float16
                )
                print("‚úÖ Modelo cargado en 8-bit")
            else:
                raise Exception("Usando fallback FP16")
                
        except Exception as e:
            print(f"‚ö†Ô∏è  8-bit fall√≥ ({e}), usando FP16...")
            # FALLBACK: FP16 sin cuantizaci√≥n
            self.model = AutoModel.from_pretrained(
                model_name,
                device_map="auto",
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            print("‚úÖ Modelo cargado en FP16 (sin cuantizaci√≥n)")
        
        self.model.eval()
    
    def compute_S_composite(self, text: str) -> float:
        """
        M√©trica S compuesta (del paper)
        S = (local_coherence √ó top5_variance) / (isotropy + 0.1)
        """
        # Tokenizar
        inputs = self.tokenizer(
            text, 
            return_tensors='pt', 
            truncation=True, 
            max_length=64,  # Reducido para velocidad
            padding=True
        )
        inputs = {k: v.to(self.model.device) for k, v in inputs.items()}
        
        # Forward pass
        with torch.no_grad():
            outputs = self.model(**inputs, output_hidden_states=True)
        
        # Embeddings √∫ltima capa
        embeddings = outputs.hidden_states[-1][0].cpu().float().numpy()
        
        # 1. LOCAL COHERENCE (gradiente sem√°ntico)
        if len(embeddings) > 1:
            cosine_sims = []
            for i in range(len(embeddings) - 1):
                sim = np.dot(embeddings[i], embeddings[i+1]) / (
                    np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i+1]) + 1e-8
                )
                cosine_sims.append(sim)
            local_coh = np.mean(cosine_sims)
        else:
            local_coh = 0
        
        # 2. TOP5 VARIANCE (concentraci√≥n espectral)
        _, s, _ = np.linalg.svd(embeddings, full_matrices=False)
        total_var = np.sum(s**2)
        top5_var = np.sum(s[:5]**2) / (total_var + 1e-8)
        
        # 3. ISOTROPY (uniformidad de normas)
        norms = np.linalg.norm(embeddings, axis=1)
        isotropy = np.std(norms) / (np.mean(norms) + 1e-8)
        
        # M√âTRICA COMPUESTA
        S = (local_coh * top5_var) / (isotropy + 0.1)
        
        return S
    
    def analyze_batch(self, sentences: list) -> dict:
        """
        Procesa batch con limpieza de memoria
        """
        results = {'S_values': [], 'texts': []}
        
        for i, sent in enumerate(sentences):
            try:
                S = self.compute_S_composite(sent)
                results['S_values'].append(S)
                results['texts'].append(sent)
                
                # Limpiar cada 5 oraciones
                if (i + 1) % 5 == 0:
                    torch.cuda.empty_cache()
                    gc.collect()
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Error en oraci√≥n {i}: {e}")
                continue
        
        return results


# ============================================
# CORPUS DE PRUEBA (ESPA√ëOL)
# ============================================
corpus = {
    'coherent': [
        "La teor√≠a cu√°ntica describe el comportamiento de part√≠culas subat√≥micas.",
        "El cambio clim√°tico afecta los ecosistemas marinos globalmente.",
        "Las redes neuronales aprenden patrones complejos de los datos.",
        "La m√∫sica cl√°sica influy√≥ profundamente la cultura europea.",
        "Los algoritmos de aprendizaje profundo transforman la inteligencia artificial.",
        "La geometr√≠a diferencial estudia las propiedades de variedades curvas.",
        "El desarrollo sostenible requiere equilibrio entre econom√≠a y medio ambiente.",
        "La evoluci√≥n biol√≥gica ocurre mediante selecci√≥n natural continua.",
    ],
    'broken': [
        "Subat√≥micas part√≠culas de comportamiento el describe cu√°ntica teor√≠a la.",
        "Globalmente marinos ecosistemas los afecta clim√°tico cambio el.",
        "Datos los de complejos patrones aprenden neuronales redes las.",
        "Europea cultura la profundamente influy√≥ cl√°sica m√∫sica la.",
        "Artificial inteligencia la transforman profundo aprendizaje de algoritmos los.",
        "Curvas variedades de propiedades las estudia diferencial geometr√≠a la.",
        "Ambiente medio y econom√≠a entre equilibrio requiere sostenible desarrollo el.",
        "Continua natural selecci√≥n mediante ocurre biol√≥gica evoluci√≥n la.",
    ]
}


# ============================================
# FUNCI√ìN PRINCIPAL DE TEST
# ============================================
def test_large_models():
    """
    Test espec√≠fico para 2.8B, 6.9B y 12B
    Validaci√≥n de transiciones T2, T3, T4
    """
    
    print("="*60)
    print("üß™ TEST VALIDACI√ìN MODELOS GRANDES")
    print("="*60)
    print("Objetivo: Confirmar transiciones en:")
    print("  T2: 2.8B  - Octava (ratio 2.00 desde 1.4B)")
    print("  T3: 6.9B  - Territorio (ratio 2.46 desde 2.8B)")
    print("  T4: 12B   - Protecci√≥n (ratio 1.74 desde 6.9B)")
    print("\nPredicciones esperadas:")
    print("  ŒîS < 0 (inversi√≥n de fase)")
    print("  p < 0.05 (significancia estad√≠stica)")
    print("  |d| > 1.5 (efecto grande)")
    print("="*60)
    
    # MODELOS A TESTEAR - SOLO LOS GRANDES
    models_to_test = [
        ('EleutherAI/pythia-2.8b', 2.8e9, True),   # T2: Octave
        ('EleutherAI/pythia-6.9b', 6.9e9, True),   # T3: Territory
        ('EleutherAI/pythia-12b', 12e9, True),     # T4: Protection
    ]
    
    results_summary = {}
    
    for model_name, n_params, use_8bit in models_to_test:
        print(f"\n{'='*60}")
        print(f"üìä MODELO: {model_name} ({n_params/1e9:.1f}B par√°metros)")
        print("="*60)
        
        try:
            # Inicializar analizador
            analyzer = OptimizedSemanticAnalyzer(model_name, use_8bit=use_8bit)
            
            # Analizar corpus
            print("üîç Analizando corpus coherente...")
            coh_results = analyzer.analyze_batch(corpus['coherent'])
            
            print("üîç Analizando corpus sintaxis rota...")
            broken_results = analyzer.analyze_batch(corpus['broken'])
            
            # Calcular estad√≠sticas
            S_coh = np.array(coh_results['S_values'])
            S_broken = np.array(broken_results['S_values'])
            
            mean_coh = np.mean(S_coh)
            mean_broken = np.mean(S_broken)
            delta_S = mean_coh - mean_broken
            
            # Test t
            t_stat, p_val = stats.ttest_ind(S_coh, S_broken)
            
            # Cohen's d
            pooled_std = np.sqrt((np.var(S_coh) + np.var(S_broken)) / 2)
            cohens_d = delta_S / (pooled_std + 1e-8)
            
            # Guardar resultados
            results_summary[model_name] = {
                'n_params': n_params,
                'S_coh': mean_coh,
                'S_broken': mean_broken,
                'ŒîS': delta_S,
                'p_value': p_val,
                'cohens_d': cohens_d,
                'inversion': delta_S < 0,
                'significant': p_val < 0.05
            }
            
            # Imprimir resultados
            print(f"\nüìà RESULTADOS:")
            print(f"  S(coherente):     {mean_coh:.4f}")
            print(f"  S(sintaxis rota): {mean_broken:.4f}")
            print(f"  ŒîS = S_coh - S_broken: {delta_S:.4f}")
            print(f"  p-value: {p_val:.6f}")
            print(f"  Cohen's d: {cohens_d:.4f}")
            
            
            
            # Limpiar memoria
            del analyzer
            torch.cuda.empty_cache()
            gc.collect()
            
        except Exception as e:
            print(f"‚ùå Error procesando {model_name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # ============================================
    # AN√ÅLISIS DE RATIOS
    # ============================================
    print("\n" + "="*60)
    print("üìä AN√ÅLISIS DE RATIOS DE SCALING")
    print("="*60)
    
    if len(results_summary) >= 2:
        # Ordenar por par√°metros
        models_sorted = sorted(results_summary.items(), key=lambda x: x[1]['n_params'])
        
        print("\nResumen de todos los modelos:")
        print("-" * 60)
        for model_name, res in models_sorted:
            model_short = model_name.split('/')[-1]
            status = "‚úÖ" if res['inversion'] and res['significant'] else "‚ö†Ô∏è"
            print(f"{status} {model_short:15s} ({res['n_params']/1e9:5.2f}B) | "
                  f"ŒîS={res['ŒîS']:+.3f} | p={res['p_value']:.5f} | d={res['cohens_d']:+.2f}")
        
        # Calcular ratios
        print("\nüî¢ RATIOS DE SCALING:")
        print("-" * 60)
        ratios = []
        for i in range(len(models_sorted) - 1):
            n1 = models_sorted[i][1]['n_params']
            n2 = models_sorted[i+1][1]['n_params']
            ratio = n2 / n1
            ratios.append(ratio)
            
            name1 = models_sorted[i][0].split('/')[-1]
            name2 = models_sorted[i+1][0].split('/')[-1]
            print(f"  {name2} / {name1} = {ratio:.3f}")
        
        # An√°lisis de ratios
        if len(ratios) >= 2:
            print("\nüìê AN√ÅLISIS ESTAD√çSTICO DE RATIOS:")
            print("-" * 60)
            mean_ratio = np.mean(ratios)
            std_ratio = np.std(ratios)
            cv = std_ratio / mean_ratio
            
            print(f"  Media: {mean_ratio:.3f}")
            print(f"  Desviaci√≥n est√°ndar: {std_ratio:.3f}")
            print(f"  Coeficiente de variaci√≥n: {cv:.2%}")
            
            # Comparaci√≥n con constantes te√≥ricas
            print("\n  Comparaci√≥n con constantes:")
            candidates = {
                'œÜ¬≤ (golden¬≤)': 2.618,
                '‚àö8': 2.828,
                'e': 2.718,
                '432/174': 2.483,
            }
            
            for name, val in candidates.items():
                diff = abs(mean_ratio - val)
                pct = (diff / val) * 100
                marker = "‚úÖ" if pct < 10 else "‚ö†Ô∏è" if pct < 20 else "‚ùå"
                print(f"    {marker} {name:15s} = {val:.3f} | diff = {diff:.3f} ({pct:.1f}%)")
            
            # Buscar mejor match individual
            print("\n  Mejor match por ratio individual:")
            for idx, r in enumerate(ratios):
                print(f"\n  Ratio {idx+1} = {r:.3f}:")
                best_match = None
                best_diff = float('inf')
                
                for name, val in candidates.items():
                    diff = abs(r - val)
                    if diff < best_diff:
                        best_diff = diff
                        best_match = (name, val)
                
                if best_match:
                    pct = (best_diff / best_match[1]) * 100
                    print(f"    ‚Üí {best_match[0]} = {best_match[1]:.3f} (error: {pct:.1f}%)")
    
    # ============================================
    # AN√ÅLISIS DE RESONANCIA MONOGRAMAS
    # ============================================
    print("\n" + "="*60)
    print("üéµ AN√ÅLISIS DE RESONANCIA CON MONOGRAMAS TdST")
    print("="*60)
    
    monograms = {
        'Âüç Territorio': 432,
        'Êöò Tiempo-Luz': 528,
        '·àê Meltha': 111,
        '‡•ê Pacificaci√≥n': 136.1,
        '·õâ Algiz': 741,
        'ÂÄô Inercia': 396,
        '·ö¶ Corte': 852,
        'Ë±ê Abundancia': 639,
        '√ë Identidad': 417,
        '‚ò© Camino': 963,
        '‚òÖ Esperanza': 999,
        '‡¶à Dulzura': 174,
    }
    
    if len(ratios) > 0:
        print("\nBuscando correspondencias frecuenciales...")
        print("-" * 60)
        
        freqs = list(monograms.values())
        
        for idx, r in enumerate(ratios):
            print(f"\nüìª Ratio r{idx+1} = {r:.3f}:")
            matches = []
            
            # Buscar pares de frecuencias que den ratio cercana
            for i, f1 in enumerate(freqs):
                for j, f2 in enumerate(freqs):
                    if i != j and f1 < f2:  # Solo ascendentes
                        freq_ratio = f2 / f1
                        diff = abs(freq_ratio - r)
                        if diff < 0.20:  # Tolerancia m√°s amplia
                            name1 = [k for k, v in monograms.items() if v == f1][0]
                            name2 = [k for k, v in monograms.items() if v == f2][0]
                            matches.append((name1, name2, f1, f2, freq_ratio, diff))
            
            # Ordenar por mejor match
            matches = sorted(matches, key=lambda x: x[5])
            
            if matches:
                print(f"  Resonancias encontradas:")
                for m in matches[:3]:  # Top 3
                    error_pct = (m[5] / r) * 100
                    marker = "üéØ" if m[5] < 0.05 else "‚úÖ" if m[5] < 0.10 else "‚ö†Ô∏è"
                    print(f"    {marker} {m[1]:18s} / {m[0]:18s}")
                    print(f"       {m[3]:6.1f} Hz / {m[2]:6.1f} Hz = {m[4]:.3f} (error: {error_pct:.1f}%)")
            else:
                print(f"  ‚ö†Ô∏è  No se encontraron resonancias claras (tolerancia ¬±0.20)")
    
    # ============================================
    # PREDICCI√ìN PR√ìXIMA TRANSICI√ìN
    # ============================================
    print("\n" + "="*60)
    print("üîÆ PREDICCI√ìN PR√ìXIMA TRANSICI√ìN (T5)")
    print("="*60)
    
    if len(models_sorted) > 0 and len(ratios) > 0:
        last_params = models_sorted[-1][1]['n_params']
        last_ratio = ratios[-1]
        mean_ratio = np.mean(ratios) if len(ratios) > 1 else last_ratio
        
        print(f"\n√öltimo modelo: {models_sorted[-1][0].split('/')[-1]} ({last_params/1e9:.1f}B)")
        print(f"√öltima ratio: {last_ratio:.3f}")
        if len(ratios) > 1:
            print(f"Media de ratios: {mean_ratio:.3f}")
        
        print("\nPredicciones T5:")
        print("-" * 60)
        
        # Predicci√≥n 1: √öltima ratio
        pred1 = last_params * last_ratio
        print(f"  Usando √∫ltima ratio ({last_ratio:.3f}): {pred1/1e9:.1f}B")
        
        # Predicci√≥n 2: Media
        if len(ratios) > 1:
            pred2 = last_params * mean_ratio
            print(f"  Usando media ({mean_ratio:.3f}): {pred2/1e9:.1f}B")
        
        # Predicci√≥n 3: Convergencia (ratio decreciente)
        if len(ratios) >= 2:
            # Ajuste lineal de ratios
            x = np.arange(len(ratios))
            slope, intercept = np.polyfit(x, ratios, 1)
            next_ratio = slope * len(ratios) + intercept
            pred3 = last_params * next_ratio
            print(f"  Convergencia lineal ({next_ratio:.3f}): {pred3/1e9:.1f}B")
        
        # Predicci√≥n 4: Constantes te√≥ricas
        print(f"\n  Referencias te√≥ricas desde {last_params/1e9:.1f}B:")
        for name, val in [('œÜ¬≤', 2.618), ('‚àö8', 2.828), ('e', 2.718)]:
            pred = last_params * val
            print(f"    √ó {name} ({val:.3f}): {pred/1e9:.1f}B")
    
    print("\n" + "="*60)
    print("‚úÖ TEST COMPLETADO")
    print("="*60)
    
    return results_summary


# ============================================
# EJECUTAR
# ============================================
if __name__ == "__main__":
    print("üöÄ Iniciando validaci√≥n modelos grandes...")
    print("‚è±Ô∏è  Tiempo estimado: ~2 horas")
    print("üíæ Aseg√∫rate de estar en GPU T4\n")
    
    # Verificar GPU
    if torch.cuda.is_available():
        print(f"‚úÖ GPU detectada: {torch.cuda.get_device_name(0)}")
        print(f"üíæ VRAM disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\n")
    else:
        print("‚ùå No se detect√≥ GPU. Este script requiere GPU.")
        exit(1)
    
    results = test_large_models()
    
    print("\nüéØ PR√ìXIMOS PASOS:")
    print("  1. Comparar con resultados previos de 6.9B")
    print("  2. Ejecutar test completo 70M-12B para secuencia total")
    print("  3. Computar Œ∫_s y œà_te para Transparencia Coloidal")